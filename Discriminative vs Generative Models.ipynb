{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression vs. Bayesian Classifier\n",
    "## Discriminative vs Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task I: Load training and test data and Implementing Bayesian Classifier and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn=-1)\n",
    "library(ggplot2)\n",
    "library(mvtnorm)\n",
    "library(reshape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain <- read.csv(file = \"Task1E_train.csv\",head=TRUE,sep=\",\") # loading training data used to train the model\n",
    "dtest <- read.csv(file = \"Task1E_test.csv\",head=TRUE,sep=\",\") # test data to test the performance of learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data to train and labels\n",
    "train.data = dtrain[-3]\n",
    "test.data = dtest[-3]\n",
    "\n",
    "train.label = dtrain[,3]\n",
    "test.label = dtest[,3]\n",
    "\n",
    "c0 <- '1'; c1 <- '-1' # class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma <- matrix(c(1, 0, 0, 1), nrow=2, ncol=2, byrow = TRUE) # shared covariance matrix\n",
    "sigma0 <- sigma;   sigma1 <- sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxilary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function that predicts class labels\n",
    "predict <- function(w, X, c0, c1){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(ifelse(sig>0.5, c1,c0))\n",
    "}\n",
    "    \n",
    "# auxiliary function that calculate a cost function\n",
    "cost <- function (w, X, T, c0){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(sum(ifelse(T==c0, 1-sig, sig)))\n",
    "}\n",
    "\n",
    "# Sigmoid function (=p(C1|X))\n",
    "sigmoid <- function(w, x){\n",
    "    return(1.0/(1.0+exp(-w%*%t(cbind(1,x)))))    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Bayesian Classifier\n",
    "These are the steps to build a bayesian Classifier:\n",
    "<ol>\n",
    "\t<li>Calculate the class priors $p(\\mathcal{C}_k)$ based on the relative number of training data in each class,</li>\n",
    "\t<li>Calculate the class means $\\mu_k$, class covariance matrices $\\mathbf{S}_k$ and shared covariance matrix $\\Sigma$ using the training data,</li>\n",
    "\t<li>Using the estimated PDF function, calculate $p(x_n|\\mathcal{C}_k)$ for each data point and each class,</li>\n",
    "\t<li>For each test sample, find the class label $\\mathcal{C}_k$ that maximizes the $p(\\mathcal{C}_k)p(x_n|\\mathcal{C}_k)$,</li>\n",
    "</ol>\n",
    "###### <span style=\"color: red\">Code used here is adopted from \"Activity.3.2.ipynb\" file form Module 3.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_classifier <- function(train.data, train.label, test.label, test.data){\n",
    "    # Class probabilities:\n",
    "    p0.hat <- sum(train.label==c0)/nrow(train.data) # total number of samples in class 0 divided by the total nmber of training data\n",
    "    p1.hat <- sum(train.label==c1)/nrow(train.data) # or simply 1 - p1.hat\n",
    "\n",
    "    # Class means:\n",
    "    mu0.hat <- colMeans(train.data[train.label==c0,])\n",
    "    mu1.hat <- colMeans(train.data[train.label==c1,])\n",
    "\n",
    "    # class covariance matrices:\n",
    "    sigma0.hat <- var(train.data[train.label==c0,])\n",
    "    sigma1.hat <- var(train.data[train.label==c1,])\n",
    "\n",
    "    # shared covariance matrix:\n",
    "    sigma.hat <- p0.hat * sigma0.hat + p1.hat * sigma1.hat \n",
    "\n",
    "    # calculate posteriors:\n",
    "    posterior0 <- p0.hat*dmvnorm(x=train.data, mean=mu0.hat, sigma=sigma.hat)\n",
    "    posterior1 <- p1.hat*dmvnorm(x=train.data, mean=mu1.hat, sigma=sigma.hat)\n",
    "\n",
    "    # calculate predictions:\n",
    "    train.predict <- ifelse(posterior0 > posterior1, c0, c1)\n",
    "    test.predict <- ifelse(p0.hat*dmvnorm(x=test.data, mean=mu0.hat, sigma=sigma.hat) > p1.hat*dmvnorm(x=test.data, mean=mu1.hat, sigma=sigma.hat), c0, c1)\n",
    "\n",
    "    test.error <- sum(test.label != test.predict)/nrow(test.data)*100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing of Logistic Regression\n",
    "These are the steps is neccesseary to build a logistic regression:\n",
    "<ol>\n",
    "\t<li>Implement sigmoid function $\\sigma(\\pmb{w}.\\mathbf{x})$, and initialize weight vector $\\pmb{w}$, learning rate $\\eta$ and stopping criterion $\\epsilon$.</li>\n",
    "\t<li>Repeat the followings until the improvement becomes negligible (i.e., $|\\mathcal{L}(\\pmb{w}^{(\\tau+1)})-\\mathcal{L}(\\pmb{w}^{(\\tau)})| \\lt \\epsilon$):\n",
    "<ol>\n",
    "\t<li>Shuffle the training data</li>\n",
    "\t<li>For each datapoint in the training data do:\n",
    "<ol>\n",
    "\t<li>$\\pmb{w}^{(\\tau+1)} := \\pmb{w}^{(\\tau)} - \\eta (\\sigma(\\pmb{w}.\\mathbf{x}) - t_n) \\pmb{x}_n$</li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression <- function(train.data, train.label, test.label, test.data){\n",
    "    tau.max <- 1000 # maximum number of iterations\n",
    "    eta <- 0.01 # learning rate\n",
    "    epsilon <- 0.01 # a threshold on the cost (to terminate the process)\n",
    "    tau <- 1 # iteration counter\n",
    "    terminate <- FALSE\n",
    "    \n",
    "    train.len <- nrow(train.data)\n",
    "    \n",
    "        ## Just a few name/type conversion to make the rest of the code easy to follow\n",
    "    X <- as.matrix(train.data) # rename just for conviniance\n",
    "    T <- ifelse(train.label==c0,0,1) # rename just for conviniance\n",
    "\n",
    "    W <- matrix(,nrow=tau.max, ncol=(ncol(X)+1)) # to be used to store the estimated coefficients\n",
    "    W[1,] <- runif(ncol(W)) # initial weight (any better idea?)\n",
    "\n",
    "    # project data using the sigmoid function (just for convenient)\n",
    "    Y <- sigmoid(W[1,],X)\n",
    "\n",
    "    costs <- data.frame('tau'=1:tau.max)  # to be used to trace the cost in each iteration\n",
    "    costs[1, 'cost'] <- cost(W[1,],X,T, c0)\n",
    "    \n",
    "    while(!terminate){\n",
    "    # check termination criteria:\n",
    "    terminate <- tau >= tau.max | cost(W[tau,],X,T, c0)<=epsilon\n",
    "    \n",
    "    # shuffle data:\n",
    "    train.index <- sample(1:train.len, train.len, replace = FALSE)\n",
    "    X <- X[train.index,]\n",
    "    T <- T[train.index]\n",
    "    \n",
    "    # for each datapoint:\n",
    "    for (i in 1:train.len){\n",
    "        # check termination criteria:\n",
    "        if (tau >= tau.max | cost(W[tau,],X,T, c0) <=epsilon) {terminate<-TRUE;break}\n",
    "        \n",
    "        Y <- sigmoid(W[tau,],X)\n",
    "            \n",
    "        # Update the weights\n",
    "        W[(tau+1),] <- W[tau,] - eta * (Y[i]-T[i]) * cbind(1, t(X[i,]))\n",
    "        \n",
    "        # record the cost:\n",
    "        costs[(tau+1), 'cost'] <- cost(W[tau,],X,T, c0)\n",
    "        \n",
    "        # update the counter:\n",
    "        tau <- tau + 1\n",
    "        \n",
    "        # decrease learning rate:\n",
    "        eta = eta * 0.999\n",
    "    }\n",
    "}\n",
    "    # Done!\n",
    "    costs <- costs[1:tau, ] # remove the NaN tail of the vector (in case of early stopping)\n",
    "\n",
    "    # the  final result is:\n",
    "    \n",
    "    w <- W[tau,]\n",
    "    \n",
    "    # calculate predictions:\n",
    "    test.predict <- predict(w, test.data, c0, c1)\n",
    "\n",
    "    test.error <- sum(test.label != test.predict)/nrow(test.data)*100\n",
    "    \n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to increase the size of training set (5 data points at a time) and calculate test errors for both \n",
    "# Bayesian Classifier and Logistic Regression models until all training\n",
    "sample_seq = seq(from = 5, to = nrow(dtrain), by = 5)\n",
    "\n",
    "\n",
    "er_test = matrix(0, nrow = length(sample_seq), ncol = 3) # empty matrix to store test errors for Bayesian Classifier\n",
    "\n",
    "\n",
    "colnames(er_test) = c('trainSetSize', 'testErrorsBayesian', 'testErrorsLR')\n",
    "mat_index = 1 # matrix index to store test errors\n",
    "for (size in sample_seq){\n",
    "   \n",
    "    er_test[mat_index, 1] = size\n",
    "    er_test[mat_index, 2] = bayesian_classifier(train.data[1:size,], train.label[1:size], test.label, test.data)\n",
    "    er_test[mat_index, 3] = logistic_regression(train.data[1:size,], train.label[1:size], test.label, test.data)\n",
    "    mat_index = mat_index + 1\n",
    "    \n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot test errors for both Bayesian Classifier and Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAv8RNTU1oaGh8\nfHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enr6+vw8PD4dm3////ccKm3AAAACXBI\nWXMAABJ0AAASdAHeZh94AAAezklEQVR4nO2dCXcp3QIFz21CRIL0//+xtwf0iJN2sHerWuvL\ngLIzqNdIbl7IAeBuwqs/AIA5QEgACSAkgAQQEkACCAkgAYQEkABCAkgAIQEkgJAAEkBIAAmY\nHlKYosZJXx8hhI/thOsHeA2CIf1koWY5YQDgJTw5pAh+ioPRd/H6exk+HrMAkBy9kBZhc3xr\nGbh3ByYkCqk4fITl8Wa/XhRvf9cXOSyKw0pxyXUWFtuT1Ho/z7fLkG1aV7ZtjkM/YX2eKV/V\nVxcWx7MX4dAdBngZaUL6qh/UrMu3j49w6mw+yhOLm/rppGNI5/fzz1psruyjdxjqhFRe3SpU\nkebfYdUdBngdSUL6qY5AP9Vdsc/wmZcvl9VFlofqVfadHz7Kk44hnd8vHhB9FUehrLmyrDzO\nDGfqkMqr+zkesj6KoNrDAC8kSUjr+sZ/KG/ii/rtYzHHe3jb6txwPvn8/rp+QPTVXFn/oVcn\npOrq6oVDeRevPQzwQpKEtAgnqnd/tp/L04OhPO+10H1/cSohLqTqzW11T+6zPJT1hgFeRZKQ\nQvv2vMnOb94O6XQlzZUtrt21q0/KstOLQEigQaKQmpM3xSOZ9dduckir9uOd3eiF18XBqD4s\nERCIkCSkLPyc316cHhflMSEN79q1nv7eZcvRkMqHR8uw6w0DvJAkIa3KZ6KrJ+9OJ28jQzo+\n2bDpVPlVv7HL6ifMD6frO1/oI6zq3NrDAC8k0dPf5R2tn+qGX/1iwvH57Nsh1U9/f7Uf5HzX\nvyJ0KB5rlbEsw8fheH3nC30ffwjVGQZ4IfeE1DzO3zY/F92cTv6OCen4A9nOswXb00nVQee7\n+Ylt+xmJReey/EAWXkyakPJd+Ts/9e/IFUeSbPVdPdSJCKn8FaHiCNR51uBQPnseVsejzPci\nZJ95N6Sv8+/jtYYBXofG0178RBXMeXFI1UOk8jdPOaiANS8O6fQQiWfdwJtX37Xblv+qfPn1\n4o8C4E5eHRLALCAkgAQQEkACCAkgAYQEkABCAkgAIQEkIGVI+yc5TxvCeeaQN4SEIzLkDSHh\niAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD\n3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuTN1JD2ABNIeuNVgiMSjsiQN4SE\nIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgM\neUNIOCJD3hASjsiQN4SEIzLkDSHhiAx5kzyk39+/O5OGcJ7iEFIc6Y9IfytJ+nuL88whbwgJ\nR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ\n8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwh\nJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlH\nZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERny\nhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEk\nHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdk\nyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKG\nkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQc\nkSFvCAlHZMgbQsIRGfKGkHBEhry5HVJW0H59GUKaoUNIcdwMKTu+yM7vXISQZugQUhyEhCMy\n5E1cSDkhvatDSHFEhFQ/NuqFtL/I7+/l8+Ddedwt+cXcDulYEUek93Q4IsXBYyQckSFvCAlH\nZMgbQsIRGfKGkHBEhrzhNxtwRIa84XftcESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQc\nkSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TI\nG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQ\ncESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByR\nIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgb\nQsIRGfImfUh/K0n6e4vzzCFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQ\ncESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByR\nIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgb\nQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBw\nRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEh\nbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMibqSHtL/P7e+VMeG+S3niV4IiEIzLkDSHh\niAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD\n3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeE\nhCMy5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGI\nDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPe\nEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQN4SE\nIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgM\neUNIOCJD3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94Q\nEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQj\nMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQNw8I6U8lSX9vcZ45\n5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlD\nSDgiQ94QEo7IkDdRIWXVi4LrFyOkGTqEFEdMSFVAdUxXL0dIM3QIKY6IkLKckN7XIaQ4boeU\n5YT0xg4hxTE1pP0Vfn+vnQvvzMNuyK/mZkhZzhHpnR2OSHHcCuncDyG9p0NIcdwMqYaQ3tUh\npDiif45ESO/pEFIchIQjMuQNv9mAIzLkDb9rhyMy5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLk\nDSHhiAx50wtpubrjughphg4hxdELKbvnCEVIM3QUQgph/L2Q8v7UnfQ+lJ/lejf5ughphg4h\nxdH7UMKZCddFSDN0FELqQkjXnElDOI93njZ0CIvq9SL85N8fIWTrvLw9/mTLOpjWaflHWO7y\nY0iHVQirw4TBtPCsHY7I0Eco49gVPW3r/y1fl6ksw6oKpn1akVTIDseQsvLkxZTBpBASjsjQ\ntqwkX4dtcVD6Kh6ul5lU5VTBtE9bHvJlnVSef5ZvrMNmymJK+iEd1oui7/WkQyUhzdB54mOk\nRfXPdap/YrDbfi7raM534bqnlQeuY2DleeFj0mJCeiHtsvoImk157o6QZug8MaRN+M6/w2fx\n1vL0OL1OqHo5clr93+TH9EnpfQCr6lHcrrxj+ncIaYbOE0M6FLe6dTiUt8LFZrvrRjN2mnBI\npw+IZ+1wnj1UxrKr7qPVz8YNozmdNrxr93oICUdkKC/u14Xi3l156/vOD8t+SM1py/Ktz/r0\ndflkw1dx0ovhrh2OyFBeHl6q57HXYfh4qH1a++nvQ/2o/mfiYjJ4sgFHZCgvn274ql6viqPO\nd+/JhvZpH+Hj/Gzerjpj4mA6ePobR2TIG34giyMy5A3/HglHZMgb/j0SjsiQN/x7JByRIW/4\nZxQ4IkPeEBKOyJA3PGuHIzLkDc/a4YgMecOzdjgiQ97wrB2OyJA3PNmAIzLkDSHhiAx5w7N2\nOCJD3hASjsiQN+2QWvfnuGuH8+whbwYhtf4p1V8hpBk6hBQHIeGIDHlDSDgiQ94QEo7IkDeE\nhCMy5A0h4YgM3Un0bxJE/8bBnxogJByRoUjGbpnhwunXriDxX2jthhT4FSGcVw1FQkhXnUlD\nOI93nhrS7zjti5z+2urpntP5vdZvE+TVu63L9d45Xuqsn97vXmvrEtUVXE+CXxHCERnKo0Jq\njj6h+7r3azmhc7mB1L6a0wkj1zq8gksQEo7IUCTDJppjR+sI03nRvNNcsHV9Ie8HlHeE3hVc\n/LBSQUgzdDRDOqUQeoeX83sjIbUvfD7vXF4Yv9a83+a1DysVhDRDRzOk1rthcNqlkLr30ELn\nv2vXSkg49zvqIY09Rhq8CP0L5908BiENHiPl/fPHPqxUENIMHd2QLj/ZMHgxVtVJ64U0fLKB\nu3Y4CRy9kIZPf1enjT3Z0H3Gu7lwSx+5dO/p70lHpNOTGfX/SfsfIaQZOnIhPYT7jyfta8j4\ngSzO64ZeSdqQNq2ONhOui5Bm6LxDSJMOG/3rSHiVhDRD5x1CSgFPNuCIDHnTD2mT5fl3yD6n\nXBchzdAhpDh6IRUPk/Jd+aTDlJIIaYYOIcXRC2kRvov/Nj+Bp79xnjzkzfDJhm1YTHzSgZBm\n6BBSHL1gsrBbhZ/yUdKE6yKkGTqEFEcvpM/i4VFWHpDWE66LkGboEFIc/btw65BtiwPTlI4I\naY4OIcXBz5FwRIa8ISQckSFvBiFtPkLIlz9TrouQZugQUhy9kA6L+k8bhe8J10VIM3QIKY5e\nSKuwLn+G9BWWE66LkGboEFIcI7/9ffrvzxDSDB1CioOQcESG7iT+j+j/VYib7757vGu3DqsJ\n10VIM3TkQhq74Q/+BlDMFfRf30f/yYbjPzfPdhOui5Bm6BDS3671zOcihMX6MOW6CGmGzlND\n+jdO+yLJ/oj+o0O6A0KaoaMWUszftWv9vbrj+QMpf2RI9z7uIqQZOpJ37caaiP8j+qfXD3uy\ngZBwXjgUSRNNfr7J9o5Igxenv//YnJ336kryYZ3fJiSc1w1F0ntiYfIf0c+7WaX4sDof0x0Q\n0gwd9ZDGHiMNXoT+hc9nPigk/tIqzsuGIol4smHwYvzw1HmV4sM6v01IOK8biiTZH9HnWTvp\nG97cHLmQRCEkHJEhbwgJR2TIG0LCERnyhl8RwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERny\nhpBwRIa8ISQckSFvCAlHZMgbQsIRGfJmakj7K/z+XjsX3pmkN14lOCLhiAx5Q0g4IkPeEBKO\nyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLk\nDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ948IqS/lCT9vcV55pA3hIQjMuQN\nIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLkDSHhiAx5Q0g4\nIkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQ\nN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h\n4YgMeUNIOCJD3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgi\nQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3\nhIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLkDSHh\niAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD\n3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94QEo7IkDeE\nhCMy5M3tkLKC9uvLENIMHUKK42ZI2fFFdn7nIoQ0Q4eQ4iAkHJEhb+IeIxHS2zqEFMfUkPbX\n+P29eja8L4+5FQsQFVKWc0R6V4cjUhyEhCMy5E1MSFn3xUUIaYYOIcUREVLWvCSkt3MIKY6I\nH8i2XhHS2zmEFMftnyNlx19p4Dcb3tIhpDj4XTsckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByR\nIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgb\nQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBw\nRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEh\nbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtC\nwhEZ8oaQcESGvCEkHJEhbwgJR2TIm4eE9IeSpL+3OM8c8oaQcESGvCEkHJEhbwgJR2TIG0LC\nERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESG\nvEke0r9/OSHNyCGkONIfkcqSCGk2DiHFQUg4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgiQ94Q\nEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQj\nMuQNIeGIDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD3jzg79oVJRHSbBxCioOQcESG\nvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW+m\nhrS/zL9/+9/fK+fDG5P0xqsERyQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtC\nwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERny5gEhFSUR0mwcQoqDkHBEhrwhJByRIW8ICUdk\nyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKG\nkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyhpBwRIa8eURI\nZUl/diYN4TzcIaQ4CAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEk\nHJEhbwgJR2TIG0LCERnyhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdk\nyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERny5iEhFSX93Zk0hPNoh5DiICQckSFvCAlH\nZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERny\nhpBwRIa8ISQckSFvCAlHZMgbQsIRGfKGkHBEhrwhJByRIW8ICUdkyJvXhPQ7yoShKR8cjuaQ\nN08JqdvIsKB/BWMplaePJal8w5ubQ0hxPCOkTiK9g88+P/ZymWtDUYwf/64fDi+fffnAGbUz\ntkZI/jwhpPJ2c77ptG5F1YX2VUeXYjoerS4P3WbKjfvy+aPR/XFneB0vCCn6QSwhxfGYkH7b\nN/7qRnO85bRuQXUh+3z83lsjn+/3XbgV9i5+6UZ743t7w4mPI+o29JfiYhnd6b61P795/Yve\nofcJRX0shHQX7dtd8406JVS/3eqoSml/81tapXT5VtN+99oN7Ob39pbTnHP1hjy203yO57cu\n3AJHPtV4Rj+h9lv705vVF//WF2TkE4r9SAjpLtohlfcdOt/g9nGppPw+XnoM1Kd/od9BOud3\nIj648/zt3ZgLXd/J25/lpQd9+85Fp+38Kbl/x/+mMO2LMHseF9K/3le+/Ujp/Cgq8nbTu9S+\nvtr6NnG863c9o+7Q8AbdOWa0b/Xdy0R8sIPbUKX1Xlxw/v3hSHF6lqY5pVPKv+OfjW4eZB6/\nUPWb1UX+EN7pwnEf2z9CuotOSL//+l/2zjfi38C5Svfmtc9b3+FTSde+zZ0ohs8IDo4Z/4aX\nGTl89m/yvUt0rqB1Xv+Kz1dUnxRZ0v7aAf3KJ9TsXfhc+le4H/vqjYnt8ahPYVY8KqTf3+FX\nc3hA+kNI7Wvb1xPn72nzv7UX7erB2L+RG3Q+OGY05+x7l2nfRgY3qN5tdSzEwUX7H8bwU71I\n6/MZ+3Tbn1Xz+ZzePV6s+7GPfVbnofYVXb14/cWO+QzmxeNCGvlf/pLeCbFf8n8XuH5u62L7\nvPVO/1qbt4ef0Mgt59oHM/pJR35KkZ9MxEc0/gmdjmI3vqbXd2K+DIR0F52Q8v6XO8/zkdty\n9Jf80jf4ytmdCw4fUzRe8/boJ9TZuniDOjuDj+3G53R24j6XwWd/9ZzB59M+beRzGbvC1qWH\nH/voxQnpLi5/+ca+y7ecSUM4qR1+IBvHc0LKLz4ylv7e4jxzyJunhZTQkb7hzc0hpDgICUdk\nyBtCwhEZ8oaQcESGvCEkHJEhbwgJR2TIG0LCERnyJj6krOD6JWb3vcV55pA30SFl5xcXmd33\nFueZQ94QEo7IkDeEhCMy5M3UkPYAE3jATVgDjkg4IkPeEBKOyJA3hIQjMuQNIeGIDHlDSDgi\nQ97wmw04IkPe8Lt2OCJD3hASjsiQN4SEIzLkDSHhiAx5Q0g4IkPeEBKOyJA3hIQjMuQNIeGI\nDHlDSDgiQ94QEo7IkDeEhCMy5A0h4YgMeUNIOCJD3hASjsiQN4SEIzLkTcqQAN4WQgJIACEB\nJICQABJASAAJICSABBASQAIICSABhASQAEICSEC6kG7/Jdb7JzpDjxrsX7/7Tn76e9OP3smO\n1/yET0iPZCFF/G3wuyeajexxg/3rd9/Jj1+4x+9krVeP/YQEMQopywlp4hIhPRyjkPLnhHRa\nms3O8bqfsdO8JqSpEJLqztNCOj1EevSQJIR0aegJd4WecrvL8meF1BsgpEkQ0sShB++cr/Y5\nt29Cupc5hTSnu5BZdrzLRUgPhZAuzDzlLtezbnfctXs4hDS+QkiTRwjpPubymw3Zs35AP8vf\nbHjKkCL8rh1AAggJIAGEBJAAQgJIACEBJICQABJASAAJICSABBASQAIIqUs4M3be5ff6HDYf\nWVhuuidusrHzr18ReMA3sUuikH6y+kqyw7jSPp+Q5gDfxCEJbtmLsCoS2S3DevyKx88HXwhp\nyOn2HsJPtszz74/iyLE+nh7C7mPsvTKKxfZcyvGNQ/X6sApVN63jXPv84r/mKHi6LJhBSEOa\nkJZhlW/r2/j6lE42+t4h69wh/Ajb5vqqsxadkNrnd0I6XRbMIKQhTUjlsWYRvorHNMdDRxnX\nId+ErP/eZ1jmh+U5lF0WFuuvXfX2Z3k167Bp37Vrn386dVlcRXNZ8IKQhjQh1bf03fZz2YS0\ny09vtd9blG/tmlIOn4vy0PKdl2dVV/bRefDVOv94atlR67LgBSENaUKqXi1Pd7tO/7VDar/X\ne5riZ71alkez5n5b91mM5vzinUPVUeuy4AXfsiHdkFZhsdnuJoRUnZBdDqk5v7qrt6qvgJA8\n4Vs2pBtSfcC4FVLvrl0Ih7w5q3/F3fOL/4qO6ufBF3w/TOEbN6Qf0vfxaYRrIa3Le2bNkw3F\nu8XDn8O6fLSzLiP5Ks9vQmqfX5x66qh1WfCCkIZ0Q1qHmMdIvae/88XxNxd257N+qmfLR85v\nP/3dXBa8IKQhvScbVqE4fNwKqfqB7Ffrwc1mWf4Yt7oDt6uuoTytCal1fufnSOfLgheElJLw\nPn9+CroQUhrKR1LFvcDVqz8OeBGElIbjI6ndqz8OeBGElIjNIoQVHb0thASQAEICSAAhASSA\nkAASQEgACSAkgAQQEkACCAkgAf8BjgD/5G+TeyMAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "er_test.m <- melt(as.data.frame(er_test), id='trainSetSize')\n",
    "ggplot(data=er_test.m, aes(x = trainSetSize, y = value, color=variable)) + geom_line(size=.75) + \n",
    "        labs(title='Learning Curve', x = \"Training Set Size\", y = \"Test Error\") +  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task III: Observations\n",
    "- A) What does happen for each classifier when the number of training data points is increased?\n",
    "    \n",
    "    Initially, when the training data points are increased from 5 to 30 (in five point steps), test error for both Bayesian Classifier and Logistic Regression is reduced to zero. After 30 data points, test error for both Bayesian Classifier and Logistic Regression increases to its maximum at 35 data points. Then, test error for both Bayesian Classifier and Logistic Regression fluctuates for different data points.\n",
    "    \n",
    "\n",
    "    \n",
    "- B) Which classifier is best suited when the training set is small, and which is best suited when the training set is big?\n",
    "    \n",
    "    In the given case, LR model is relatively more suitable for both small and big training set sizes. This can be seen from the error rate plot. For all training data points sizes, test error for LR model is less than or equal to error for Bayesian Classifier.\n",
    "\n",
    "    \n",
    "- C) Justify your observations in previous questions (III.a & III.b) by providing some speculations and possible reasons. \n",
    "    \n",
    "    * Both models performed well even though we had only two parameters. Possible reason could be that the data is easily separable by single decision boundary. </li>\n",
    "    * From mini-batch 5 to 140 both classifiers have same prediction and error rate. </li>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
